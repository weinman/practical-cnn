function net = initializeCharacterCNN()

f=1/100 ; % Scaling factor for randomized weights 

net.layers = {} ;

% Initial conv/pool layers
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{ f*randn(5,5,1,20, 'single'), ...
                                         zeros(1, 20, 'single') }}, ...
                           'stride', 1, ...
                           'pad', 0) ;

net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [2 2], ...
                           'stride', 2, ...
                           'pad', 0) ;
% Secondary conv/pool layers
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{ f*randn(5,5,20,50, 'single'), ...
                                          zeros(1,50,'single') }}, ...
                           'stride', 1, ...
                           'pad', 0) ;

net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [2 2], ...
                           'stride', 2, ...
                           'pad', 0) ;
% Tertiary conv/ReLU layers
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{ f*randn(4,4,50,500, 'single'), ...
                                         zeros(1,500,'single') }}, ...
                           'stride', 1, ...
                           'pad', 0) ;

net.layers{end+1} = struct('type', 'relu') ;

% Final conv-cum-fully-connected layer
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{ f*randn(2,2,500,26, 'single'), ...
                                         zeros(1,26,'single')}}, ...
                           'stride', 1, ...
                           'pad', 0) ;
% Softmax Loss layer
net.layers{end+1} = struct('type', 'softmaxloss') ;

net = vl_simplenn_tidy(net) ;
